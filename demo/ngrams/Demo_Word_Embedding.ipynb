{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Demo\n",
    "This demo is taken from Geoffrey Hinton's Coursera course on neural networks. The objective is to learn feature representations of words given a body of text separated into four word sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Alice\n",
    "Also best to start Julia with multiple threads for processing speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Base.Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and display a sample of the data\n",
    "The data is stored in the demo folder of the Alice package in .jld format. Load the data using the **`load_ngrams`** function.  \n",
    "  \n",
    "There are 4 sets of data in the Dict:\n",
    "- \"vocab\" - vector containing the vocabulary of 250 words\n",
    "- \"train_data\" - array containing 372,550 four-grams for training\n",
    "- \"valid_data\" - array contains 46,568 four-grams for cross validation\n",
    "- \"test_data\" - array contains 46,568 four-grams for testing\n",
    "\n",
    "Each column of the training, validation and test data arrays is a four-gram. And each four-gram is expressed as integer references to the vocabulary. E.g. the column vector `[193, 26, 249, 38]` is the four-gram containing the 193<sup>rd</sup>, 26<sup>th</sup>, 249<sup>th</sup> and 38<sup>th</sup> word in the vocabulary in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data, vocab = load_ngrams();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function display_rand_ngrams(data, vocab, num_display)\n",
    "    num_ngrams = size(data, 2)\n",
    "    displaywords = vocab[data[:, rand(1:num_ngrams, num_display)]]\n",
    "    for ngram in 1:num_display\n",
    "        str = \"\"\n",
    "        for w in displaywords[:, ngram]\n",
    "            str *= \"$w \"\n",
    "        end\n",
    "        @printf(\"|  %-25s\", str)\n",
    "        mod(ngram, 4) == 0 && @printf(\"|\\n\")\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  nt know who he           |  and just like that       |  's just going to         |  want to do it            |\n",
      "|  a new game .             |  is this about ?          |  less and less .          |  they want their children |\n",
      "|  not for me .             |  for our country .        |  should not come .        |  did nt know that         |\n",
      "|  same to me ;             |  i do nt know             |  does the time go         |  not too much .           |\n",
      "|  been very good ,         |  has to get them          |  house , he said          |  other set to go          |\n",
      "|  well , it was            |  a man come in            |  think about it .         |  it , too .               |\n",
      "|  said it should not       |  the show will go         |  game had come .          |  nt like war .            |\n"
     ]
    }
   ],
   "source": [
    "display_rand_ngrams(train_data, vocab, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \"words\" include punctuation marks e.g. full stop, comma, colon and some words are split e.g. \"didn't\" is split into \"did\" and \"nt\". So the n-grams haven't been selected as particlularly representative of characteristics of the words. It also doesn't look (to me) like a sequence of four words is enough to really convey meaning.  \n",
    "\n",
    "But we are just going to press on and see if the volume of data (i.e. 372,550 four-grams to train on) is enough for the model to find meaningful structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep the data for training\n",
    "The model is going to take the first three words in the four-gram as inputs and the fourth word as the target. I.e. the model is going to learn to predict the fourth word. So we're going to split the data sets into `_input` and `_target` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_input = train_data[1:3, :]\n",
    "train_target = train_data[4, :]\n",
    "val_input = valid_data[1:3, :]\n",
    "val_target = valid_data[4, :]\n",
    "test_input = test_data[1:3, :]\n",
    "test_target = test_data[4, :];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Build neural network\n",
    "The 2<sup>nd</sup> layer (1<sup>st</sup> hidden layer) is a word embedding layer that creates a feature vector for each input word. These feature vectors become the models \"understanding\" of characteristics of each word. A key feature of this model is that no word characteristics are explicitly told to the model e.g. we don't tell the model that a particular word is a verb or a particular word relates to sports. Any characteristics of the words are learned by the model through the context provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neural Network\n",
       "Training Data Dimensions - (3,372550)\n",
       "Layers:\n",
       "Layer 1 - Alice.InputLayer{Int32}, Dimensions - (3,100)\n",
       "Layer 2 - Alice.WordEmbeddingLayer{Float32}, Dimensions - (150,100)\n",
       "Layer 3 - Alice.FullyConnectedLayer{Float32}, Activation - Alice.logistic, Dimensions - (200,100)\n",
       "Layer 4 - Alice.SoftmaxOutputLayer{Float32,Int32}, Dimensions - (250,100)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed so that we can replicate results\n",
    "srand(1234)\n",
    "\n",
    "# Counts\n",
    "num_words = 3\n",
    "vocab_size = length(vocab)\n",
    "\n",
    "# Data Container\n",
    "databox = Data(train_input, train_target, val_input, val_target)\n",
    "\n",
    "# Input Layer\n",
    "batch_size = 100\n",
    "input = InputLayer(databox, batch_size)\n",
    "\n",
    "# Word Embedding Layer\n",
    "num_feats = 50\n",
    "embed = WordEmbeddingLayer(Float32, size(input), vocab_size, num_feats)\n",
    "\n",
    "# Fully Connected 1\n",
    "fc_dim = 200\n",
    "fc = FullyConnectedLayer(Float32, size(embed), fc_dim, init = Normal(0, 0.01))\n",
    "\n",
    "# Softmax output\n",
    "output = SoftmaxOutputLayer(Float32, databox, size(fc), vocab_size, init = Normal(0, 0.01))\n",
    "\n",
    "# Build Network\n",
    "net = NeuralNet(databox, [input, embed, fc, output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:48:31 : Epoch 1, last batch training error (with regⁿ) - 2.892\n",
      "14:48:38 : Epoch 2, last batch training error (with regⁿ) - 2.551\n",
      "14:48:45 : Epoch 3, last batch training error (with regⁿ) - 2.388\n",
      "14:48:53 : Epoch 4, last batch training error (with regⁿ) - 2.300\n",
      "14:49:01 : Epoch 5, last batch training error (with regⁿ) - 2.249\n",
      "\n",
      "Coffee break:\n",
      "Training error (with regⁿ) - 2.665  |  Training accuracy - 35.5\n",
      "Validation error (without regⁿ) - 2.712  |  Validation accuracy - 35.0\n",
      "\n",
      "14:49:20 : Epoch 6, last batch training error (with regⁿ) - 2.225\n",
      "14:49:27 : Epoch 7, last batch training error (with regⁿ) - 2.211\n",
      "14:49:35 : Epoch 8, last batch training error (with regⁿ) - 2.196\n",
      "14:49:42 : Epoch 9, last batch training error (with regⁿ) - 2.176\n",
      "14:49:49 : Epoch 10, last batch training error (with regⁿ) - 2.154\n",
      "\n",
      "Completed Training:\n",
      "Training error (with regⁿ) - 2.497  |  Training accuracy - 37.8\n",
      "Validation error (without regⁿ) - 2.606  |  Validation accuracy - 36.5\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters\n",
    "α = 0.1            # learning rate\n",
    "μ = 0.9            # momentum parameter\n",
    "num_epochs = 10    # total number of epochs\n",
    "\n",
    "# Train\n",
    "train(net, num_epochs, α, μ, nesterov = false, shuffle = false, last_train_every = 1, full_train_every = 5, val_every = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Some results\n",
    "The functions **`display_nearest_words`** and **`predict_next_word`** are provided in Alice.  \n",
    "\n",
    "#### `display_nearest_words` \n",
    "outputs the words considered most similar (using Euclidean distance of the learned feature vectors) by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word       distance\n",
      "--------   --------\n",
      "four       1.43\n",
      "three      1.78\n",
      "two        1.89\n",
      "several    2.15\n",
      "million    2.38\n"
     ]
    }
   ],
   "source": [
    "display_nearest_words(embed, vocab, \"five\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word       distance\n",
      "--------   --------\n",
      "week       1.57\n",
      "day        2.06\n",
      "season     2.09\n",
      "year       2.15\n",
      "days       2.18\n"
     ]
    }
   ],
   "source": [
    "display_nearest_words(embed, vocab, \"night\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `predict_next_word`\n",
    "outputs the top suggestions for the target word after a given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string                     probability\n",
      "---------------------      -----------\n",
      "john is the best           0.138\n",
      "john is the same           0.103\n",
      "john is the right          0.076\n",
      "john is the last           0.041\n",
      "john is the president      0.038\n",
      "john is the first          0.032\n",
      "john is the one            0.025\n",
      "john is the man            0.025\n",
      "john is the time           0.024\n",
      "john is the only           0.024\n"
     ]
    }
   ],
   "source": [
    "predict_next_word(net, vocab, (\"john\", \"is\", \"the\"), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Sorted Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$, 's, ), ,, -, --, ., :, ;, ?, a, about, after, against, ago, all, also, american, among, an, and, another, any, are, around, as, at, back, be, because, been, before, being, best, between, big, both, business, but, by, called, can, case, center, children, city, come, companies, company, could, country, court, day, days, department, did, director, do, does, down, dr., during, each, end, even, every, family, federal, few, first, five, for, former, found, four, from, game, general, get, go, going, good, government, group, had, has, have, he, her, here, high, him, his, home, house, how, i, if, in, including, into, is, it, its, john, just, know, last, law, left, less, life, like, little, long, made, make, man, many, market, may, me, members, might, million, money, more, most, mr., ms., much, music, my, national, never, new, next, night, no, not, now, nt, of, off, office, officials, old, on, one, only, or, other, our, out, over, own, part, people, percent, place, play, police, political, president, program, public, put, right, said, same, say, says, school, season, second, see, set, several, she, should, show, since, so, some, state, states, still, street, such, take, team, than, that, the, their, them, then, there, these, they, think, this, those, though, three, through, time, times, to, today, too, two, under, united, university, until, up, us, use, used, very, want, war, was, way, we, week, well, were, west, what, when, where, which, while, white, who, will, with, without, women, work, world, would, year, years, yesterday, york, you, your, "
     ]
    }
   ],
   "source": [
    "for w in sort(vocab)\n",
    "    print(\"$w, \")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
